{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ed5955",
   "metadata": {},
   "source": [
    "# Kernel Development & Optimizations with Triton\n",
    "\n",
    "Welcome to this hands-on workshop! [OpenAI Triton](https://github.com/triton-lang/triton) is an open-source programming language designed to simplify GPU programming for high-performance tasks, particularly in AI applications, which has been supported by AMD GPUs. This workshop will demonstrate how to set up the Triton development environment and optimize Triton Kernel perofrmance on AMD MI GPU. \n",
    "\n",
    "### Agenda:\n",
    "- Set up Triton Development Enviroment üñêÔ∏è‚öíÔ∏è\n",
    "\n",
    "- Kernel Development Examples (Vector-Add) \n",
    "\n",
    "- Kernel Development Hands-on (Online-Softmax) \n",
    "\n",
    "    - The Origin of Online-Softmax Algorithm\n",
    "\n",
    "    - Online-Softmax Kernel Hands-on üñêÔ∏è‚öíÔ∏è\n",
    "\n",
    "    - Performance Benchmark Result Visualization \n",
    "\n",
    "    - Skimmed through the advanced Fused-Softmax üß†\n",
    " \n",
    "\n",
    "\n",
    "## 1. Set up Triton development enviroment \n",
    " \n",
    "\n",
    "### Step1: Access AMD MI GPU \n",
    "In this workshop, we will use the **MI GPU** cloud instance from AMD Developer Cloud. Please access your GPU instance link, which has been provided by this workshop.\n",
    "\n",
    "### Step2: Run ROCm Official Container on GPU card \n",
    "In this workshop, we will work on the pre-built ROCm PyTorch image, which pulled with command 'docker pull rocm/pytorch:rocm7.0.2_ubuntu24.04_py3.12_pytorch_release_2.8.0'. It has integrated Pytorch with AMD ROCm software stack succeffully. Developers can also try other ROCm as the base image docker hub page from [docker images](https://hub.docker.com/r/rocm/pytorch/tags) if need. \n",
    "\n",
    "### Step3: Install OpenAI Triton\n",
    "\n",
    "####    Uninstall the old Triton\n",
    "It is strongly recommended to use the latest version Triton in your project, because AMD and other vendors are updating their optimization passes and algorithms frequently in [OpenAI Triton](https://github.com/triton-lang/triton), which can help improve your Triton kernel performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fea56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c3edf",
   "metadata": {},
   "source": [
    "#### Install OpenAI Triton from source codes\n",
    "The detailed steps to install Triton have been listed here. If meeting any questions or issues when building Triton,please submit them in [Triton Issues](https://github.com/triton-lang/triton/issues).   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# # Remove existing Triton folder if it exists\n",
    "# if [ -d \"triton\" ]; then\n",
    "#     echo \"Removing existing triton directory...\"\n",
    "#     rm -rf triton\n",
    "# fi\n",
    "\n",
    "# # Clone Triton repo\n",
    "# git clone https://github.com/triton-lang/triton.git\n",
    "\n",
    "# # Install dependencies and Triton from source (non-editable install)\n",
    "# cd triton\n",
    "# pip install -r python/requirements.txt\n",
    "# pip install .\n",
    "\n",
    "pip install triton\n",
    "pip install matplotlib\n",
    "# if not docker based, pls uncomment below line:\n",
    "# pip install torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 --index-url https://download.pytorch.org/whl/rocm6.4\n",
    "pip list | grep -E 'triton|torch'\n",
    "\n",
    "# Pls ignore the incompatible error which will not  affect the examples' exection in this notebook.\n",
    "# Confirm it install successfully by showing 'Successfully installed triton-xxx'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4b7a4",
   "metadata": {},
   "source": [
    "## 2. Kernel Development Examples (Vector-Add)\n",
    "\n",
    "Once Triton is installed on the machine successfully, we can validate whether it can work well or not on AMD GPU machine. By running the below vector-add sample through python, we can find that Triton kernel can give the sample result with Torch APIs, which means Triton can work well on AMD GPUs.\n",
    "\n",
    "By running the below Autotune vector-add sample through python, we can find that Triton kernel can be tuned with triton.autotune API and get better result on AMD GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfc6db-0381-4218-84c3-5eb2b3427f37",
   "metadata": {},
   "source": [
    "### Basic Vector-Add kernel (no autotune, fixed BLOCK_SIZE) üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764119f2-0b07-4e2c-b65f-c5c24521662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "# ============================================================\n",
    "# Version 1: Basic Vector-Add kernel (no autotune, fixed BLOCK_SIZE)\n",
    "# ============================================================\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel_basic(x_ptr,  # *Pointer* to first input vector.\n",
    "                     y_ptr,  # *Pointer* to second input vector.\n",
    "                     output_ptr,  # *Pointer* to output vector.\n",
    "                     n_elements,  # Size of the vector.\n",
    "                     BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "                     ):\n",
    "    # There are multiple 'programs' processing different data. We identify which program we are here:\n",
    "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # Note that offsets is a list of pointers:\n",
    "    \n",
    "    # Each program handles a contiguous range of elements.\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a multiple of BLOCK_SIZE.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "\n",
    "    # Compute output = x + y\n",
    "    output = x + y\n",
    "\n",
    "    # Write the results back to DRAM.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb58845-c520-4dbf-a634-85dfd763204c",
   "metadata": {},
   "source": [
    "### Autotuned Vector-Add kernel (w/ autotune of BLOCK_SIZE configuration) üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fee74-010d-47b0-b0fc-f3c5b00608f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Version 2: Autotuned Vector-Add kernel\n",
    "# ============================================================\n",
    "\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({\"BLOCK_SIZE\": 4}, num_stages=3, num_warps=8),\n",
    "        triton.Config({\"BLOCK_SIZE\": 4}, num_stages=4, num_warps=4),\n",
    "        triton.Config({\"BLOCK_SIZE\": 2}, num_stages=3, num_warps=8),\n",
    "        triton.Config({\"BLOCK_SIZE\": 2}, num_stages=4, num_warps=4),\n",
    "    ],\n",
    "    key=['SIZE_CATEGORY'],\n",
    ")\n",
    "@triton.jit\n",
    "def add_kernel_autotune(x_ptr,  # *Pointer* to first input vector.\n",
    "                        y_ptr,  # *Pointer* to second input vector.\n",
    "                        output_ptr,  # *Pointer* to output vector.\n",
    "                        n_elements,  # Size of the vector.\n",
    "                        SIZE_CATEGORY: tl.constexpr, # Category of the problem. \n",
    "                        BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "                        ):\n",
    "    # Identify the program ID along the 1D grid.\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # Compute start offset for this program.\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # Create a mask for safe memory operations.\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    # Load inputs from memory.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "\n",
    "    # Perform vector addition.\n",
    "    output = x + y\n",
    "\n",
    "    # Store the results.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e9104-2932-42cb-bb18-f70ba2632a6c",
   "metadata": {},
   "source": [
    "### Wrapper Python function to call the kernel üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4264a2-1748-4867-9911-9f54c71aa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Wrapper function to launch either basic or autotuned kernel\n",
    "# ============================================================\n",
    "# We can now use the below function to compute the element-wise sum of two `torch.tensor` objects and test its correctness:\n",
    "\n",
    "USE_AUTOTUNE = False #  User control switch: False and True to enable or disable autotuning\n",
    "\n",
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    # Allocate output tensor\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE\n",
    "    n_elements = output.numel()\n",
    "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
    "    # It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    # NOTE:\n",
    "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
    "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n",
    "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
    "    # add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    if USE_AUTOTUNE:\n",
    "        print(\"[INFO] Using autotuned kernel...\")\n",
    "        size_category = triton.next_power_of_2(n_elements)\n",
    "        add_kernel_autotune[grid](x, y, output, n_elements,size_category)\n",
    "    else:\n",
    "        print(\"[INFO] Using basic kernel (no autotune)...\")\n",
    "        add_kernel_basic[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "\n",
    "    # Return the output tensor.\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dbd044-1020-4f66-92b5-97525bfed964",
   "metadata": {},
   "source": [
    "### Give inputs & Test üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc19a3d-36da-442e-ba9f-0f08bbb34356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Test section\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "    size = 98432\n",
    "    x = torch.rand(size, device=DEVICE)\n",
    "    y = torch.rand(size, device=DEVICE)\n",
    "\n",
    "    # Compute with PyTorch for reference\n",
    "    output_torch = x + y\n",
    "\n",
    "    # Compute with Triton\n",
    "    output_triton = add(x, y)\n",
    "\n",
    "    print(output_torch)\n",
    "    print(output_triton)\n",
    "\n",
    "    # Compare results\n",
    "    diff = torch.max(torch.abs(output_torch - output_triton))\n",
    "    print(f\"The maximum difference between torch and triton is {diff}\")\n",
    "\n",
    "    assert torch.allclose(output_torch, output_triton, atol=1e-3, rtol=1e-3), \\\n",
    "        f\"Accuracy mismatch! Max diff = {diff}\"\n",
    "\n",
    "    print(\"‚úÖ Triton kernel vector-add verification success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4b5b9",
   "metadata": {},
   "source": [
    "## 3. Kernel Development Hands-on (Online-Softmax)\n",
    "\n",
    "### 3.1 The Origin of Online-Softmax Algorithm\n",
    "\n",
    "The softmax function, often used in classification CNN models and even Transformer based LLM models,converts raw output scores or logits, into probabilities by taking the exponential of each value and normalizing these values by dividing by the sum of all the exponentials. This process ensures that the output values are in the range (0,1) and sum up to 1, making them interpretable as probabilities. PyTorch has implemented it as [a standard API](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html). \n",
    "\n",
    "Definition of function $y = Softmax(x)$ is:\n",
    "\n",
    "$$\n",
    "y_i = \\frac{e^{x_i}}{\\sum_{j=1}^{V} e^{x_j}} \\tag{1}\n",
    "$$\n",
    "\n",
    "where $x,y \\in \\mathbb{R}^V$.\n",
    "\n",
    "#### 3.1.1 Naive version - Safe-Softmax\n",
    "\n",
    "To achieve the numerical stability, we need to subtract the maximum value of the row vector from each input element before taking their exponentials. So the definition changes to:\n",
    "\n",
    "$$\n",
    "y_i = \n",
    "\\frac{e^{\\left(x_i - \\max_{k=1}^V x_k\\right)}}\n",
    "     {\\sum_{j=1}^V e^{\\left(x_j - \\max_{k=1}^V x_k\\right)}} \\tag{2}\n",
    "$$\n",
    "\n",
    "where $x,y \\in \\mathbb{R}^V$. This is called `Safe Softmax`.\n",
    "\n",
    "According to the softmax algorithm definition `(Equation 2)`, we implemented the naive version Triton kernel. To get the maximum data and the corresponding sum of all the exponentials, 2 for-loops are implemented in this version kernel, and there is still 1 for-loop to calculate the final softmax result. So total 3 loops are used in this kernel. The algorithm of `Safe Softmax` (from [this paper](https://arxiv.org/pdf/1805.02867)) is:\n",
    "\n",
    "![safe_softmax](./assets/safe_softmax_algo.png)\n",
    "\n",
    "Assuming that we need to test this kernel performance on an 8192x8192 tensor. We will conduct the calculation like this:\n",
    "\n",
    "![softmax_naive](./assets/softmax_naive_50p.png)\n",
    "\n",
    "- The block size of col dimension is 256.\n",
    "- We allocate one program per row of the input tensor, which means the grid size is (n_rows,) where n_rows equals number of rows of input tensor.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively process the data blocks of current row, to calculate the maximum value $m_k$ of current row. This is 1st for-loop.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively process the data blocks of current row, to calculate the denominator (sum of exponentials) value $d_j$ of current row. This is 2nd for-loop.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively process the data blocks of current row, to calculate the final softmax value $y_i$ of current row. This is 3rd for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a0b46",
   "metadata": {},
   "source": [
    "#### 3.1.2 Online-softmax version \n",
    "Triton language is easy to implement an algorothm for GPU developpers. In order to have the better performance of current kernel, we can first figure out whether there is a more efficient algorithm/solution. If so, we had better try the new algorithm in our Triton Kernel. To reduce the memory access caused by 3 for-loops in naive SoftMax algorithm, a new algorithm of on-line softmax has been proposed in [this paper](https://arxiv.org/pdf/1805.02867). The online softmax algorithm is:\n",
    "\n",
    "\n",
    "![online_softmax](./assets/online_softmax_algo.png)\n",
    "\n",
    "\n",
    "The `Algorithm 3` calculates maximum value $m_j$ and denominator $d_j$ in a single for-loop, which can remove redundant memory overhead in `Algorithm 2`.\n",
    "\n",
    "We will conduct the online calculation like this:\n",
    "\n",
    "- The block size of col dimension is 256.\n",
    "- We allocate one program per row of the input tensor, which means the grid size is (nrows,) where nrows equals number of rows of input tensor.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively process the data blocks of current row, to calculate the maximum value $m_k$ and the denominator (sum of exponentials) value $d_j$ of current row. This is 1st for-loop.\n",
    "- The program instance (thread block) scans one row of the tensor and iteratively process the data blocks of current row, to calculate the final softmax value $y_i$ of current row. This is 2nd for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe41a46-03ea-4f0f-8369-07983b918156",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Kernel Development Hands-on (Online-Softmax)\n",
    "\n",
    "### Here are 5 things you need to do\n",
    "\n",
    "Understand how the calculation is conducted, with the help of above `Algorithm 3`.\n",
    "-  Write your own code in (**\"YOUR CODE HERE 1Ô∏è‚É£\"** in below code cell) to set up the program ID.\n",
    "-  Write your own code in 1st for-loop (**\"YOUR CODE HERE 2Ô∏è‚É£\"** in below code cell) to set up the mask to make program only access elements with $offsets$ less than $n\\_cols$. \n",
    "-  Write your own code in 1st for-loop (**\"YOUR CODE HERE 3Ô∏è‚É£ \"** in below code cell) to use tl.load() to load input data from global memory. It's combination of rows offset (pid*row_stride) and cols offset (n_cols).\n",
    "-  Write your own code in 1st for-loop (**\"YOUR CODE HERE 4Ô∏è‚É£ \"** in below code cell) to finish the calculation of maximum value $m_k$.\n",
    "-  Write your own code in 1st for-loop (**\"YOUR CODE HERE 5Ô∏è‚É£Ô∏è \"** in below code cell) to calculate the sum of exponentials as denominator (sum of exponentials) value $d_j$.\n",
    "\n",
    "Tips: Check triton language APIs in this [link](https://triton-lang.org/main/python-api/triton.language.html).\n",
    "\n",
    "After the code is finished, run the code cell below to check the result correctness and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d608d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "# === Teaching mode flag ===\n",
    "BEGINNER_MODE = False # True\n",
    "\n",
    "# === Triton kernel ===\n",
    "@triton.jit\n",
    "def online_softmax_kernel(in_ptr, output_ptr, row_stride, n_cols, BLOCK_SIZE: tl.constexpr):\n",
    "    placeholder = tl.zeros([1], dtype=tl.float32) #for quiz only\n",
    "    \n",
    "    # === YOUR CODE HERE 1Ô∏è‚É£ ===: set up the program ID\n",
    "    # [TODO] Replace placeholder[0] with your expression for 'pid'.\n",
    "    pid = tl.program_id(0)\n",
    "    \n",
    "    in_max = -float('inf')\n",
    "    in_exp_sum = 0.0\n",
    "\n",
    "    # --- Step 1: Compute global max (Algorithm 3, Line 4) ---\n",
    "    for col_idx in range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n",
    "        \n",
    "        # === YOUR CODE HERE 2Ô∏è‚É£ ===: set up the mask to make program only access elements with offsets less than n_cols \n",
    "        # [TODO] Replace placeholder[0] with your expression for 'mask'.\n",
    "        mask = offsets < n_cols\n",
    "\n",
    "        # === YOUR CODE HERE 3Ô∏è‚É£ ===: use tl.load() to load input data from global memory\n",
    "        # [TODO] Replace placeholder[0] with your expression for 'in_data'. It's combination \n",
    "        # of rows offset (pid*row_stride) and cols offset (offsets).\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + offsets, mask=mask, other=-float('inf'))\n",
    "        \n",
    "        # === YOUR CODE HERE 4Ô∏è‚É£ ===:  Calculate the max value (use tl.max() api) of current block data\n",
    "        # according to Line 4 in Algorithm 3, get the new global max value by comparing with the\n",
    "        # previous global max value in_max (use tl.maximum() api).\n",
    "        # [TODO] Replace placeholder[0] with your expression for 'in_max_new'.\n",
    "        in_max_new = tl.maximum(in_max, tl.max(in_data, axis=0))\n",
    "        \n",
    "        # === YOUR CODE HERE 5Ô∏è‚É£Ô∏è ===: # Calculate the sum of exponentials (use tl.exp() and tl.sum() apis)\n",
    "        # of current block data according to Line 5 in Algorithm 3, and update the global sum of\n",
    "        # exponentials in_exp_sum according to Line 5 in Algorithm 3\n",
    "        # [TODO] Replace placeholder[0] with your expression for 'in_exp_sum'.\n",
    "        in_exp_sum = in_exp_sum * tl.exp(in_max - in_max_new) + tl.sum(tl.exp(in_data - in_max_new), axis=0)\n",
    "\n",
    "        in_max = in_max_new\n",
    "\n",
    "    # --- Step 2: Normalize and write output ---\n",
    "    for col_idx in range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = col_idx + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < n_cols\n",
    "        in_data = tl.load(in_ptr + pid * row_stride + offsets, mask=mask)\n",
    "        in_exp = tl.exp(in_data - in_max)\n",
    "        tl.store(output_ptr + pid * row_stride + offsets, in_exp / in_exp_sum, mask=mask)\n",
    "\n",
    "\n",
    "# === Host-side wrapper ===\n",
    "def run_online_softmax(x, output_triton):\n",
    "    \"\"\"\n",
    "    Wrap the kernel call.\n",
    "    If BEGINNER_MODE=True, show hint text before running kernel.\n",
    "    Capture exceptions and provide teaching hints if code is incomplete.\n",
    "    \"\"\"\n",
    "\n",
    "    if BEGINNER_MODE:\n",
    "        print(\"\\nüß© [Beginner Mode Enabled]\")\n",
    "        print(\"Make sure the following lines in kernel are correctly filled:\")\n",
    "        print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "        print(\"1Ô∏è‚É£  pid = tl.program_id(0)\")\n",
    "        print(\"2Ô∏è‚É£  mask = offsets < n_cols \")\n",
    "        print(\"3Ô∏è‚É£  in_data = tl.load(in_ptr + pid * row_stride + offsets, mask=mask, other=-float('inf')) \")\n",
    "        print(\"4Ô∏è‚É£  in_max_new = tl.maximum(in_max, tl.max(in_data, axis=0))\")\n",
    "        print(\"5Ô∏è‚É£  in_exp_sum = in_exp_sum * tl.exp(in_max - in_max_new) + tl.sum(tl.exp(in_data - in_max_new), axis=0)\")       \n",
    "        print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\")\n",
    "\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    try:\n",
    "        online_softmax_kernel[(n_rows,)](\n",
    "            x,\n",
    "            output_triton,\n",
    "            x.stride(0),\n",
    "            n_cols=n_cols,\n",
    "            BLOCK_SIZE=256\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"\\n‚ùå [Error Detected During Kernel Launch]\")\n",
    "        print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "        print(f\"‚ö†Ô∏è Exception Type: {type(e).__name__}\")\n",
    "        print(f\"üí¨ Message: {e}\")\n",
    "        print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "        print(\"üí° Possible causes:\")\n",
    "        print(\"  - The placeholders[0] have not been replaced with actual expressions.\")\n",
    "        print(\"  - Syntax errors or missing parameters in Triton kernel.\")\n",
    "        print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\")\n",
    "        if not BEGINNER_MODE:\n",
    "            print(\"Please wait for the workshop presenter to introduce the assignment!\")\n",
    "            print(\"Tip: You can also enable BEGINNER_MODE = True at the top of the script for guided hints.\")\n",
    "        # Prevent crash ‚Äî just return silently\n",
    "        return\n",
    "\n",
    "\n",
    "# === Main test ===\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "output_torch = torch.softmax(x, dim=-1)\n",
    "output_triton = torch.empty_like(x)\n",
    "\n",
    "run_online_softmax(x, output_triton)\n",
    "\n",
    "# === Check correctness ===\n",
    "print(\"\\nChecking correctness...\")\n",
    "print(\"\\n\")\n",
    "if torch.allclose(output_torch, output_triton, atol=1e-3, rtol=1e-3):\n",
    "    print(\"‚úÖ Success! The output of the Triton kernel-based Online Softmax aligns with the PyTorch version!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Accuracy mismatch detected. Triton output does not match PyTorch output.\")\n",
    "    # print(f\"Maximum difference: {torch.max(torch.abs(output_torch - output_triton))}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd81406b-29a3-4024-8ee6-b084ba74b553",
   "metadata": {},
   "source": [
    "### Performacne Measurement via 'Latency(ms)' üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be198bab-212c-4f1b-b615-ef5e2429dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols = x.shape\n",
    "output_triton = torch.empty_like(x)\n",
    "\n",
    "# Warm up the no-tune version\n",
    "online_softmax_kernel[(n_rows,)](\n",
    "        x,\n",
    "        output_triton,\n",
    "        x.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE=256\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# latency of online softmax kernel \n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "online_softmax_kernel[(n_rows,)](\n",
    "        x,\n",
    "        output_triton,\n",
    "        x.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE=256\n",
    ")\n",
    "end_event.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "assert torch.allclose(output_torch, output_triton, atol=1e-3, rtol=1e-3), \"Accuracy mismatch: The maximum difference between torch and triton is \" \\\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}'\n",
    "print(\"\\n\")\n",
    "print(f'Online Softmax Triton Version Elapsed: {elapsed_time_ms:.3f}ms')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30edd8a3-2ea9-4350-aa50-a872389e40f0",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3. Visualize the performance via perf_report feature and Compare with Torch Naive Kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c8bcc-81c2-41ce-bcb7-4cd27afdfd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "\n",
    "# --- PyTorch Online Softmax ---\n",
    "def softmax_torch(x: torch.Tensor, dim=-1):\n",
    "    \"\"\"\n",
    "    Compute softmax using PyTorch built-in function.\n",
    "    Output is the same shape as input.\n",
    "    \"\"\"\n",
    "    output = F.softmax(x, dim=dim)\n",
    "    return output\n",
    "    \n",
    "\n",
    "# --- Helper to run Triton Autotune ---\n",
    "def softmax_triton(x: torch.Tensor):\n",
    "    n_rows, n_cols = x.shape\n",
    "    output = torch.empty_like(x)\n",
    "    online_softmax_kernel[(n_rows,)](\n",
    "        x,\n",
    "        output,\n",
    "        x.stride(0),\n",
    "        n_cols,\n",
    "        BLOCK_SIZE=256\n",
    "    )\n",
    "    # print(\"triton output:\", output)\n",
    "    return output\n",
    "\n",
    "# --- Triton Benchmark ---\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(60, 100)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg`\n",
    "        line_names=[\"Triton Online-Softmax\", \"Torch\"],  # label name for the lines\n",
    "        styles=[('blue', 'solid'), ('orange', 'dashdot')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"Online-Softmax Performance Benchamrk\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch.softmax(x, dim=-1), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_triton(x), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    \n",
    "    # Calculate bandwidth: 2 * (read + write) * size / time\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "# --- Run benchmark ---\n",
    "benchmark.run(show_plots=True, print_data=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e0ce9-5bc7-4cbe-b1aa-333093deb048",
   "metadata": {},
   "source": [
    "### 3.4 Skim through the advanced Fused-Softmax \n",
    "This example demonstrates how to implement a fused softmax kernel using Triton, with architectural awareness for AMD ROCm/CDNA backends.\n",
    "\n",
    "OpenAI Triton provided a reference softmax sample codes with the name of \"fused-softmax\". Based on online softmax, it continued to simplify the maxmumim data calculation, which can remove 1 for-loop. it also ask the compiler to use more threads per row by increasing the number of warps, which is often tuned for better performance. and finally it improved the kernel lauching scheme by the GPU hardware properties, which can have the higher GPU kernel occupancy and better performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df425375",
   "metadata": {},
   "source": [
    "The calculation is conducted like this:\n",
    "\n",
    "![fused_softmax](./assets/softmax_fused_50p.png)\n",
    "\n",
    "- It processes an entire row as a single data block, meaning a larger BLOCK_SIZE, and loads the whole row into SRAM once instead of repeatedly accessing global memory. \n",
    "- The maximum data calculation is a parallel reduction across the data block, which is efficient in Triton.\n",
    "- It also ask the compiler to use more threads per row by increasing the number of warps, which is often tuned for better performance.\n",
    "- As the program uses more resources, the number of programs is less than n_rows. So each program is assigned one or more rows of the input matrix.\n",
    "- The register usage is evaluated by warmup run, and the calculated occupancy helps to choose the proper number of programs (thread block). \n",
    "\n",
    "  \n",
    "#### Here are 2 things you need to do\n",
    "1. Understand the methodology of architecture-awared algorithm optimization.\n",
    "2. Check the performance improvement and re-run the benchmark and visualization with Torch kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.runtime import driver\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "\n",
    "def is_cdna():\n",
    "    return is_hip() and triton.runtime.driver.active.get_current_target().arch in ('gfx940', 'gfx941', 'gfx942',\n",
    "                                                                                   'gfx90a', 'gfx908', 'gfx950')\n",
    "\n",
    "@triton.jit\n",
    "def fused_softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr,\n",
    "                   num_stages: tl.constexpr):\n",
    "    # starting row of the program\n",
    "    row_start = tl.program_id(0)\n",
    "    row_step = tl.num_programs(0)\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # The stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "        # The block size is the next power of two greater than n_cols, so we can fit each\n",
    "        # row in a single block\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n",
    "        # Subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "        # Note that exponentiation in Triton is fast but approximate (i.e., think __expf in CUDA)\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "        softmax_output = numerator / denominator\n",
    "        # Write back output to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        output_ptrs = output_row_start_ptr + col_offsets\n",
    "        tl.store(output_ptrs, softmax_output, mask=mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd4b0ab",
   "metadata": {},
   "source": [
    "### To tune the kernel, we first get some resource properties of our GPU by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = driver.active.utils.get_device_properties(DEVICE.index)\n",
    "NUM_SM = properties[\"multiprocessor_count\"]\n",
    "NUM_REGS = properties[\"max_num_regs\"]\n",
    "SIZE_SMEM = properties[\"max_shared_mem\"]\n",
    "WARP_SIZE = properties[\"warpSize\"]\n",
    "target = triton.runtime.driver.active.get_current_target()\n",
    "kernels = {}\n",
    "print(f\"NUM_SM: {NUM_SM}, NUM_REGS: {NUM_REGS}, SIZE_SMEM: {SIZE_SMEM}, WARP_SIZE: {WARP_SIZE}, target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dcd565",
   "metadata": {},
   "source": [
    "### Then we setup the kernel launch configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d48352",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.randn(8192, 8192, device=DEVICE)\n",
    "output_torch = torch.softmax(x, dim=-1)\n",
    "n_rows, n_cols = x.shape\n",
    "# Allocate output\n",
    "y = torch.empty_like(x)\n",
    "\n",
    "# The block size of each loop iteration is the smallest power of two greater than the number of columns in `x`\n",
    "BLOCK_SIZE = triton.next_power_of_2(n_cols*2)\n",
    "\n",
    "# Another trick we can use is to ask the compiler to use more threads per row by\n",
    "# increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "num_warps = 8\n",
    "\n",
    "# Number of software pipelining stages.\n",
    "num_stages = 4 if SIZE_SMEM > 200000 else 2\n",
    "\n",
    "print(f\"BLOCK_SIZE: {BLOCK_SIZE}, num_warps: {num_warps}, num_stages: {num_stages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6bbb86",
   "metadata": {},
   "source": [
    "### The occupancy of the kernel is limited by register usage. To maximize the occupancy, let's warmup the kernel to get register usage, and calculate the proper programs number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-compile kernel to get register usage and compute thread occupancy.\n",
    "kernel = fused_softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                   num_stages=num_stages, num_warps=num_warps, grid=(1, ))\n",
    "kernel._init_handles()\n",
    "n_regs = kernel.n_regs\n",
    "size_smem = kernel.metadata.shared\n",
    "\n",
    "if is_hip():\n",
    "    # NUM_REGS represents the number of regular purpose registers. On CDNA architectures this is half of all registers available.\n",
    "    # However, this is not always the case. In most cases all registers can be used as regular purpose registers.\n",
    "    # ISA SECTION (3.6.4 for CDNA3)\n",
    "    # VGPRs are allocated out of two pools: regular VGPRs and accumulation VGPRs. Accumulation VGPRs are used\n",
    "    # with matrix VALU instructions, and can also be loaded directly from memory. A wave may have up to 512 total\n",
    "    # VGPRs, 256 of each type. When a wave has fewer than 512 total VGPRs, the number of each type is flexible - it is\n",
    "    # not required to be equal numbers of both types.\n",
    "    if is_cdna():\n",
    "        NUM_GPRS = NUM_REGS * 2\n",
    "\n",
    "    # MAX_NUM_THREADS represents maximum number of resident threads per multi-processor.\n",
    "    # When we divide this number with WARP_SIZE we get maximum number of waves that can\n",
    "    # execute on a CU (multi-processor)  in parallel.\n",
    "    MAX_NUM_THREADS = properties[\"max_threads_per_sm\"]\n",
    "    max_num_waves = MAX_NUM_THREADS // WARP_SIZE\n",
    "    occupancy = min(NUM_GPRS // WARP_SIZE // n_regs, max_num_waves) // num_warps\n",
    "else:\n",
    "    occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "    \n",
    "occupancy = min(occupancy, SIZE_SMEM // size_smem)\n",
    "num_programs = NUM_SM * occupancy\n",
    "\n",
    "num_programs = min(num_programs, n_rows)\n",
    "\n",
    "print(f\"n_regs: {n_regs}, size_smem: {size_smem}, occupancy: {occupancy}, num_programs: {num_programs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d5775",
   "metadata": {},
   "source": [
    "### Now everything is ready, we can verify the kernel's correctness and benchmark it, as we did in previous kernel versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a number of persistent programs.\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start_event.record()\n",
    "kernel[(num_programs, 1, 1)](y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE, num_stages)\n",
    "end_event.record()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "assert torch.allclose(output_torch, y, atol=1e-2, rtol=1e-2), \"Accuracy mismatch:The maximum difference between torch and triton is \" \\\n",
    "      f'{torch.max(torch.abs(output_torch - y))}'\n",
    "print(\"‚úÖThe output of the Triton kernel-based Fused Softmax aligns with the PyTorch version!\")\n",
    "print(f'Fused Softmax Triton Version Elapsed: {elapsed_time_ms:.3f}ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311be852-0deb-444a-b5e4-083d168e617d",
   "metadata": {},
   "source": [
    "\n",
    "# Let's re-run the benchmark and visualization !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c763c6e-51e6-4e17-b46e-9ba0dd653e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "\n",
    "# --- PyTorch Online Softmax ---\n",
    "def softmax_torch(x: torch.Tensor, dim=-1):\n",
    "    \"\"\"\n",
    "    Compute softmax using PyTorch built-in function.\n",
    "    Output is the same shape as input.\n",
    "    \"\"\"\n",
    "    output = F.softmax(x, dim=dim)\n",
    "    return output\n",
    "    \n",
    "\n",
    "# --- Helper to run Triton Autotune ---\n",
    "def softmax_triton(x: torch.Tensor):\n",
    "    n_rows, n_cols = x.shape\n",
    "    output = torch.empty_like(x)\n",
    "    kernel[(num_programs, 1, 1)](\n",
    "        y, \n",
    "        x, \n",
    "        x.stride(0), \n",
    "        y.stride(0), \n",
    "        n_rows, \n",
    "        n_cols, \n",
    "        BLOCK_SIZE, \n",
    "        num_stages)\n",
    "    return output\n",
    "\n",
    "# --- Triton Benchmark ---\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],  # argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(60, 100)],  # different possible values for `x_name`\n",
    "        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n",
    "        line_vals=['triton', 'torch'],  # possible values for `line_arg`\n",
    "        line_names=[\"Triton Fused-Softmax\", \"Torch Softmax\"],  # label name for the lines\n",
    "        styles=[('blue', 'solid'), ('orange', 'dashdot')],  # line styles\n",
    "        ylabel=\"GB/s\",  # label name for the y-axis\n",
    "        plot_name=\"Fused-Softmax Performance Benchamrk\",  # name for the plot. Used also as a file name for saving the plot.\n",
    "        args={'M': 4096},  # values for function arguments not in `x_names` and `y_name`\n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch.softmax(x, dim=-1), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_triton(x), rep=10, quantiles=quantiles\n",
    "        )\n",
    "    \n",
    "    # Calculate bandwidth: 2 * (read + write) * size / time\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms)\n",
    "\n",
    "# --- Run benchmark ---\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f798046",
   "metadata": {},
   "source": [
    "\n",
    "## Summary \n",
    "Triton simplifies high-performance GPU kernel development. Through this workshop, developer has already know how to develop and optimize Triton kernel on AMD GPUs. \n",
    "\n",
    "If developer would like to study more about OpenAI Triton itself, OpenAI Triton [official document](https://triton-lang.org/main/index.html) can be very useful. You can find more information about AMD Triton from [this document](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/optimizing-triton-kernel.html) and [this blog](https://rocm.blogs.amd.com/software-tools-optimization/kernel-development-optimizations-with-triton-on-/README.html).\n",
    "\n",
    "We hope that this workshop will encourage you to tune, test, and contribute to Triton on AMD GPUs, and help us shape the future of AI acceleration.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a5f42-e445-42b9-ac94-4829cf719515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
